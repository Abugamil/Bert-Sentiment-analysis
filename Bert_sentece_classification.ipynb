{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bert sentece classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1pOL9QFYkOR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from nltk import word_tokenize, sent_tokenize\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import re\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaIZLd4PvzFP"
      },
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sViFIxVqf7dS",
        "outputId": "fd8ae6a5-9ea4-4a1e-93ab-884293f61636"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYsfk2wzf-bZ"
      },
      "source": [
        "path = '/content/drive/My Drive/'\n",
        "path_model='/content/drive/My Drive/rubert'\n",
        "path_dataset = os.path.join(path, \"dataset\")\n",
        "path_data = os.path.join(path, \"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHhrLxReNDcD"
      },
      "source": [
        "import re\n",
        "def preprocess_text_tags(text):\n",
        "    if isinstance(text, (int, float)):\n",
        "      return ''\n",
        "    text = text.lower().replace(\"ё\", \"е\")\n",
        "    text = re.sub('((www\\.[^\\s]+)|(http[s]?://[^\\s]+))','url', text)     \n",
        "    text = re.sub('@[^\\s]+','user', text)\n",
        "    text = re.sub('\\w+@[a-zA-Z_]+?\\.[a-zA-Z]{2,4}','email', text)   \n",
        "    text = re.sub('(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)','hashtag', text)\n",
        "    text = re.sub('(?:(?:\\d+,?)+(?:\\.?\\d+)?)','num', text)\n",
        "    #text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
        "    text = re.sub(' +',' ', text)\n",
        "    return ' '.join(text.split()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlwtryiANKaT"
      },
      "source": [
        "#stops = set(stopwords.words(\"russian\"))\n",
        "\n",
        "def clean_text(data):\n",
        "    delstops = True\n",
        "    simple_filter = True\n",
        "#    del12gram = True  # True = убираем обрывки слов в 1-2 символа\n",
        "        \n",
        " #   if delstops:\n",
        " #       data = \" \".join([w for w in data.split() if w not in stops])\n",
        "    \n",
        "    if simple_filter:\n",
        "        data = preprocess_text_tags(data)\n",
        "    \n",
        "#    if del12gram:\n",
        "#        data = \" \".join([w for w in data.split() if len(w) > 2])\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FmKkWglmPSA"
      },
      "source": [
        "def tag2num(tag):\n",
        "    \"\"\"\n",
        "    document convert to vector (sum of token)\n",
        "    \"\"\"\n",
        "    ans=0\n",
        "    if tag=='PSTV':\n",
        "        ans=1\n",
        "    elif tag=='NGTV':\n",
        "        ans=-1\n",
        "    return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuXXiGTSfpow"
      },
      "source": [
        "def toxic2num(tag):\n",
        "    ans=0\n",
        "    if tag==1.0:\n",
        "        ans=0\n",
        "    elif tag==0.0:\n",
        "        ans=1\n",
        "    return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO0RelqSmhr4"
      },
      "source": [
        "def toxic_dataset():\n",
        "    path_no1 = os.path.join(path_dataset, 'toxic.csv')\n",
        "    df = pd.read_csv(path_no1)      \n",
        "    df['clear_text'] = df['comment'].map(clean_text)\n",
        "    df['tone'] = df['toxic'].apply(toxic2num)\n",
        "    df=df[['clear_text','tone']]\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23uDvkHDcMkM"
      },
      "source": [
        "def tone_dataset():\n",
        "    path_no1 = os.path.join(path_dataset, 'tone_unfiltered.csv')\n",
        "    df = pd.read_csv(path_no1)\n",
        "    df['clear_text'] = df['text'].map(clean_text)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUSYF2MZYkOh"
      },
      "source": [
        "def marked_dataset():\n",
        "    path_no1 = os.path.join(path_data, 'marked_tonal.csv')\n",
        "    df = pd.read_csv(path_no1)\n",
        "    df['clear_text'] = df['MESSAGE'].map(clean_text)\n",
        "    df.rename(columns={\"Neutral\": \"tone\"},inplace=True)\n",
        "    df.loc[df['tone']==2,'tone']=1\n",
        "    #df=df[df.tone<2]\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3bTp0tVcIkC"
      },
      "source": [
        "def check_dataset():\n",
        "    path_no1 = os.path.join(path_data, 'check_tonal.csv')\n",
        "    df = pd.read_csv(path_no1)\n",
        "    df['clear_text'] = df['MESSAGE'].map(clean_text)\n",
        "    df.rename(columns={\"Neutral\": \"tone\"},inplace=True)\n",
        "    #df=df[df.tone<2]\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SzrnRIPYkOh"
      },
      "source": [
        "def open_dataset():\n",
        "    #dtype = {'clear_text': str, 'tone': np.int64}\n",
        "    path_no1 = os.path.join(path_dataset, 'telecom_total.csv')\n",
        "    df = pd.read_csv(path_no1)\n",
        "    df['clear_text'] = df['clear_text'].map(clean_text)\n",
        "    #df.rename(columns={\"col\": \"clear_text\", \"code\": \"tone\"},inplace=True)\n",
        "    #df=df[[\"clear_text\",\"tone\"]]\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rxLfK19ou24"
      },
      "source": [
        "def predict_dataset():\n",
        "    path_no1 = os.path.join(path_data, 'to_mark_negative.csv')\n",
        "    df = pd.read_csv(path_no1)\n",
        "    df['clear_text'] = df['MESSAGE'].map(clean_text)\n",
        "    #df=df[df.spam==0]\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzq-NnSl-Gb_"
      },
      "source": [
        "def marked_topics():\n",
        "    path_no1 = os.path.join(path_dataset, 'marked_topics.csv')\n",
        "    df = pd.read_csv(path_no1)\n",
        "    df['clear_text'] = df['text'].map(clean_text)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpjI9JkvHJqI"
      },
      "source": [
        "df=marked_topics()\n",
        "df['clear_text'].replace('', np.nan, inplace=True)\n",
        "df.dropna(subset=['clear_text'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKN-L3ezHiXI",
        "outputId": "80398e99-f5f2-4b3d-9df4-e02d466e15de"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHPXKBLYYkOm",
        "outputId": "ae253406-c7b6-46ab-dc43-c4714c78f15c"
      },
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from transformers import BertModel\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 100, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        #self.bert = BertModel.from_pretrained(path_model)\n",
        "        self.bert = AutoModel.from_pretrained(path_model)\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def mean_pooling(self, model_output, attention_mask):\n",
        "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "        return sum_embeddings / sum_mask\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        #print(\"output shape=\"+str(outputs.shape))\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        sentence_embeddings = self.mean_pooling(outputs, attention_mask)\n",
        "        #last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "        #print(\"last_hidden shape=\"+str(last_hidden_state_cls))\n",
        "        # Feed input to classifier to compute logits\n",
        "        #logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return sentence_embeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.48 s, sys: 269 ms, total: 1.74 s\n",
            "Wall time: 1.63 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euJyM9E5IBMG"
      },
      "source": [
        "    # Instantiate Bert Classifier\n",
        "model = BertClassifier(freeze_bert=True)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJMeZbQHKQS1"
      },
      "source": [
        "MAX_LEN = 512\n",
        "#from pytorch_transformers import BertTokenizer\n",
        "#from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_model)\n",
        "#tokenizer = BertTokenizer.from_pretrained(path_model)\n",
        "\n",
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    data1=[sent for sent in data]\n",
        "    encoded_inputs = tokenizer(data1, padding='max_length' ,return_tensors=\"pt\", max_length=MAX_LEN, truncation=True)\n",
        "    return encoded_inputs.get('input_ids'),encoded_inputs.get('attention_mask')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2_gw107Fstb"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "test_inputs, test_masks = preprocessing_for_bert(df.clear_text)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=1)\n",
        "model.eval()\n",
        "\n",
        "all_logits = []\n",
        "#all_logits=torch.tensor((), dtype=torch.float16)\n",
        "\n",
        "# For each batch in our test set...\n",
        "for batch in test_dataloader:\n",
        "    # Load batch to GPU\n",
        "    b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids.to(device)\n",
        "    b_attn_mask.to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = model(b_input_ids, b_attn_mask)\n",
        "    all_logits.append(logits.cpu().numpy()[0])\n",
        "      #all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    #\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4uDvNyNEigY"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4S7ytrgorgQ"
      },
      "source": [
        "num_clusters = 2\n",
        "clustering_model = AgglomerativeClustering(n_clusters=num_clusters)\n",
        "clustering_model.fit(all_logits)\n",
        "cluster_assignment = clustering_model.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJxJYXg_p7nC",
        "outputId": "05889284-5656-4516-da76-6e69a9d9c2df"
      },
      "source": [
        "clusters = [[] for _ in range(len(cluster_assignment))]\n",
        "for sent_id, cluster_label in enumerate(cluster_assignment):\n",
        "    clusters[cluster_label].append(df.clear_text[sent_id])\n",
        "clusters.sort(key=lambda x:len(x), reverse=True)\n",
        "\n",
        "# Ouput\n",
        "\n",
        "cnt_gourps = 0\n",
        "text = \"\"\n",
        "for c in range(len(clusters)):\n",
        "    if clusters[c]:\n",
        "        text += \"\\n\" + \"-\"*50 + \"\\n\"\n",
        "        text += \"Cluster:%d\\n\"%c\n",
        "        text += \"\\n\".join(clusters[c])\n",
        "        if len(clusters[c])>=2:\n",
        "            cnt_gourps += 1 \n",
        "print(cnt_gourps)\n",
        "path_txt = os.path.join(path_data, 'test_cluter.txt')\n",
        "with open(path_txt, \"w\") as f:\n",
        "    f.write(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXZ10jY9m9ZF"
      },
      "source": [
        "df.drop_duplicates(inplace=True)\n",
        "df2.drop_duplicates(inplace=True)\n",
        "df3.drop_duplicates(inplace=True)\n",
        "df4.drop_duplicates(inplace=True)\n",
        "df5.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iGab-BM4PZZ",
        "outputId": "368a43bc-577a-403e-9ac4-ab38bb96e5ad"
      },
      "source": [
        "print(df.clear_text[df.tone  == 0].count())\n",
        "print(df.clear_text[df.tone  == 1].count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2413\n",
            "2606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dAiFRBinFYv",
        "outputId": "e1368b31-8876-406b-ef92-a9876940c02b"
      },
      "source": [
        "print(df2.clear_text[df2.tone  == -1].count())\n",
        "print(df2.clear_text[df2.tone  == 0].count())\n",
        "print(df2.clear_text[df2.tone  == 1].count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5232\n",
            "33213\n",
            "2440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojS98KAHgLcV",
        "outputId": "ac1838ad-fa40-462c-bc09-acba1e9d0c50"
      },
      "source": [
        "print(df3.clear_text[df3.tone  == 0].count())\n",
        "print(df3.clear_text[df3.tone  == 1].count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4826\n",
            "9586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPTm5OGKeUXy",
        "outputId": "1e59c518-4a26-4ec8-90ef-75d74aad7882"
      },
      "source": [
        "print(df4.clear_text[df4.tone  == 0].count())\n",
        "print(df4.clear_text[df4.tone  == 1].count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "816\n",
            "2184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya9YkZylUnbJ",
        "outputId": "1dd45872-7190-4e33-badc-7b6427069082"
      },
      "source": [
        "print(df5.clear_text[df5.tone  == 0].count())\n",
        "print(df5.clear_text[df5.tone  == 1].count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "416\n",
            "1295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Koncb-EHnRL2"
      },
      "source": [
        "df_negative=df.clear_text[df.tone  == 0]\n",
        "df2_negative=df2.clear_text[df2.tone  == -1]\n",
        "df3_negative=df3.clear_text[df3.tone  == 0]\n",
        "df4_negative=df4.clear_text[df4.tone  == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0RDTPWingY5"
      },
      "source": [
        "negative=pd.concat([df_negative,df2_negative,df3_negative,df4_negative])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s6gVyzDowbN"
      },
      "source": [
        "df_neutral=df.clear_text[df.tone  == 1]\n",
        "df2_neutral=df2.clear_text[df2.tone  == 0]\n",
        "df3_neutral=df3.clear_text[df3.tone  == 1]\n",
        "df4_neutral=df4.clear_text[df4.tone  == 1]\n",
        "neutral=pd.concat([df_neutral,df2_neutral,df3_neutral,df4_neutral])\n",
        "neutral.drop_duplicates(inplace=True)\n",
        "#neutral=pd.concat([df_neutral,df3_neutral])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5oC2c_ho8gJ"
      },
      "source": [
        "num_instances=len(neutral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZKQB3TcpN45"
      },
      "source": [
        "msk = [False]*num_instances\n",
        "msk = np.random.rand(num_instances)<0.3\n",
        "bln_neutral=neutral[msk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-QU-ydUBkoc"
      },
      "source": [
        "neutralx=df4.clear_text[df4.tone  == 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZbO-o79BqZE"
      },
      "source": [
        "bln_neutral=pd.concat([bln_neutral,neutralx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD6hwnDmqFM_"
      },
      "source": [
        "df_neutral=pd.DataFrame(bln_neutral)\n",
        "df_negative=pd.DataFrame(negative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecju5eMbqFaH"
      },
      "source": [
        "df_neutral['tone']=1\n",
        "df_negative['tone']=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok_0XSY2qFep"
      },
      "source": [
        "dfx=pd.concat([df_neutral,df_negative])\n",
        "num_instances=len(dfx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsFgCYDVd7Em"
      },
      "source": [
        "train=dfx\n",
        "test=df5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gScaNiI7TPb"
      },
      "source": [
        "DATA_COLUMN = 'clear_text'\n",
        "LABEL_COLUMN = 'tone'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS8_Bo4kMsXK",
        "outputId": "07bb9ad5-92dc-4239-c895-21b014d26d96"
      },
      "source": [
        "print(len(train))\n",
        "print(len(test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23100\n",
            "1706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOxVbGC2MtMq",
        "outputId": "7c8bcbc0-f8ae-4ef0-c7d3-065a2f2e6511"
      },
      "source": [
        "print(train.clear_text[train.tone  == 0].count())\n",
        "print(train.clear_text[train.tone  == 1].count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11453\n",
            "11647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf44PkCtMvT0",
        "outputId": "540b1678-2830-4b32-dc91-e153300e0251"
      },
      "source": [
        "print(test.clear_text[test.tone  == 0].count())\n",
        "print(test.clear_text[test.tone  == 1].count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "415\n",
            "1291\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmLJDywP7mhY"
      },
      "source": [
        "X=df.clear_text.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT6uOs_ON4dx"
      },
      "source": [
        "X_train, X_val=train['clear_text'],test['clear_text']\n",
        "y_train, y_val=train['tone'],test['tone']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTyMz8jwYkOj",
        "outputId": "c1f8f530-7b3d-4e20-d25f-9a56baac1df0"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGRmeL5HYkOk"
      },
      "source": [
        "MAX_LEN = 300\n",
        "#from pytorch_transformers import BertTokenizer\n",
        "#from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_model)\n",
        "#tokenizer = BertTokenizer.from_pretrained(path_model)\n",
        "\n",
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    data1=[sent for sent in data]\n",
        "    encoded_inputs = tokenizer(data1, padding='max_length' ,return_tensors=\"pt\", max_length=MAX_LEN, truncation=True)\n",
        "    return encoded_inputs.get('input_ids'),encoded_inputs.get('attention_mask')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7YjAEJ-YkOl",
        "outputId": "d9e06c6a-f420-4ac3-c67e-9aa238451f85"
      },
      "source": [
        "# Concatenate train data and test data\n",
        "all_tweets = np.concatenate([train.clear_text.values, test.clear_text.values])\n",
        "\n",
        "# Encode our concatenated data\n",
        "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
        "\n",
        "# Find the maximum length\n",
        "max_len = max([len(sent) for sent in encoded_tweets])\n",
        "print('Max length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length:  2927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHzmqSt7YkOl",
        "outputId": "b32011f2-2d16-4281-fb0d-0fad969e0458"
      },
      "source": [
        "# Specify `MAX_LEN`\n",
        "\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  талгар алматинской обл. инагда скорост прападает и зависает\n",
            "Token IDs:  [101, 15459, 9013, 76799, 47787, 7792, 852, 14181, 132, 2789, 3091, 2235, 21342, 31957, 22861, 851, 17162, 1828, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Tokenizing data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsUX2J8xYkOm"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train.to_numpy())\n",
        "val_labels = torch.tensor(y_val.to_numpy())\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7scno_cYkOn"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=1e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    warmup_steps = len(train_dataloader) * 2   ### https://www.kaggle.com/snnclsr/learning-rate-schedulers#get_linear_schedule_with_warmup (total_samples // bs) * 20\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=warmup_steps, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JssjUHHZYkOo"
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    best_valid_loss =  0.630309 #float('inf')\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'learn rate':^12} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*82)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {scheduler.get_last_lr()[0]:^2.10f} |  {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*82)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            if val_loss < best_valid_loss:\n",
        "              best_valid_loss = val_loss\n",
        "              torch.save(bert_classifier.state_dict(),'/content/drive/My Drive/tonal_model.pt') \n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {'-':^12} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*82)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kngJwuFd6aUz"
      },
      "source": [
        "#from transformers import AutoModel\n",
        "#model = AutoModel.from_pretrained(path_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2us9zNsWYkOp"
      },
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=50)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=50, evaluation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHhduU2Ba8P6"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(bert_classifier.named_parameters())\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:5]:\n",
        "   print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "for p in params[5:21]:\n",
        "   print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "for p in params[-4:]:\n",
        "   print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnmf6W8l7JF_"
      },
      "source": [
        "#inputs = tokenizer(\"Ты мне нравишься. Я тебя люблю\", return_tensors=\"pt\")\n",
        "#outputs = model(**inputs)\n",
        "#outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoxoLCnuYkOq"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTR7W7yNYkOq"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_model(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    #preds = probs[:, 1]\n",
        "    #print(probs)\n",
        "    y_pred = np.argmax(probs, axis=1)\n",
        "    #accuracy = accuracy_score(y_true, y_pred)\n",
        "    #print(f'Accuracy: {accuracy*100:.2f}%')    \n",
        "    #print(metrics.confusion_matrix(y_pred, y_true))\n",
        "    print(metrics.classification_report(y_pred, y_true, digits=3)) \n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWq2ubWZUYQc",
        "outputId": "776ca9ac-846b-473d-9524-64add496bee2"
      },
      "source": [
        "bert_classifier.load_state_dict(torch.load('/content/drive/My Drive/tonal_model.pt', map_location=device))\n",
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_model(probs, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.817     0.660     0.730       514\n",
            "           1      0.864     0.936     0.899      1192\n",
            "\n",
            "    accuracy                          0.853      1706\n",
            "   macro avg      0.841     0.798     0.814      1706\n",
            "weighted avg      0.850     0.853     0.848      1706\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KTGpdfoYkOt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}